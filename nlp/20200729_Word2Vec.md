# Word2Vec

##  개요

문서를 수치화함에 있어 Bag of word(BOW)나 TF-IDF를 사용했고, 의미를 부여하기 위해 Word Embedding을 사용하였다. Word Embedding은 classification등의 특적 목적을 달성하기 위해 그때마다 학습하는 방식이어서 사후적으로 결정되고 특정 목적에 한정된다. 

그러나 Word2Vec은 방대한 양의 코퍼스를 학습하여 단어들이 어떤 관계를 갖도록 벡터화하는 기술이다. 특정 목적에 맞게 벡터화하는 것이 아닌, 목적에 상관없이 범용적으로 사용할 수 있도록 벡터화시킨다. Word2Vec은 다음의 성질을 갖는다.

- 사후에 학습되는 것이 아닌 사전에 학습하여 단어들을 수치 벡터로 표현한다.
- 단어의 주변 단어들(Context)을 참조하여 해당 단어를 수치화한다. 주변 단어들을 참조하므로 distributed representation이라 한다.

Word2Vec의 종류에는 CBOW와 Skip-Gram 두가지가 있다.



### Continuous Bag-of-Words(CBOW) 소개

CBOW는 주변 단어들을 입력받아 해당 단어가 나오도록 학습한다. 

![image-20200729094856081](C:%5CUsers%5Csoohan%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20200729094856081.png)

이 때 우리가 얻고자 하는 단어 hurt의 Word2Vec vector는 위 네트워크의 Hidden layer에 해당한다. 주변 문장들을 모두 넣어줘야 하나의 단어에 대한 Word2Vec이 생성된다.



### Skip-Gram 소개

Skip-Gram은 해당 단어를 입력 받아 주변 단어가 나오도록 학습한다. 단어 하나만으로 Word2Vec Layer를 출력할 수 있으므로 편하다는 장점이 있다.

![image-20200729100059721](C:%5CUsers%5Csoohan%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20200729100059721.png)